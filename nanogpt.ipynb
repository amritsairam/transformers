{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters  1115394\n"
     ]
    }
   ],
   "source": [
    "with open ('/Users/sairam/Downloads/ng-video-lecture-master/input.txt','r',) as f:\n",
    "    text=f.read()\n",
    "\n",
    "print('length of dataset in characters ', len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi={ch:i for i , ch in enumerate(chars)}\n",
    "itos={i:ch for i , ch in enumerate(chars)}\n",
    "encode=lambda s: [stoi[c] for c in s ]#this is a function which applies the stoi dictionary mapping to the input string\n",
    "decode=lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data=torch.tensor(encode(text),dtype=torch.long)#text is our dataset and data is containing encoding values of the text\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "validation_data=data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([18]) the output is 47\n",
      "when the input is tensor([18, 47]) the output is 56\n",
      "when the input is tensor([18, 47, 56]) the output is 57\n",
      "when the input is tensor([18, 47, 56, 57]) the output is 58\n",
      "when the input is tensor([18, 47, 56, 57, 58]) the output is 1\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1]) the output is 15\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15]) the output is 47\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the output is 58\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context=x[:t+1]\n",
    "    target=y[t]\n",
    "    print(f'when the input is {context} the output is {target}')#----1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1003854])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target : 43\n",
      "when input is [24, 43] the target : 58\n",
      "when input is [24, 43, 58] the target : 5\n",
      "when input is [24, 43, 58, 5] the target : 57\n",
      "when input is [24, 43, 58, 5, 57] the target : 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target : 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target : 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target : 39\n",
      "when input is [44] the target : 53\n",
      "when input is [44, 53] the target : 56\n",
      "when input is [44, 53, 56] the target : 1\n",
      "when input is [44, 53, 56, 1] the target : 58\n",
      "when input is [44, 53, 56, 1, 58] the target : 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target : 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target : 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target : 1\n",
      "when input is [52] the target : 58\n",
      "when input is [52, 58] the target : 1\n",
      "when input is [52, 58, 1] the target : 58\n",
      "when input is [52, 58, 1, 58] the target : 46\n",
      "when input is [52, 58, 1, 58, 46] the target : 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target : 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target : 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target : 46\n",
      "when input is [25] the target : 17\n",
      "when input is [25, 17] the target : 27\n",
      "when input is [25, 17, 27] the target : 10\n",
      "when input is [25, 17, 27, 10] the target : 0\n",
      "when input is [25, 17, 27, 10, 0] the target : 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target : 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target : 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target : 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size=4# how many independent sequences will we process in parallel\n",
    "block_size=8# the maximum context length for prediction ,when we train the transformer we randomly pick setences which has a length of 8\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #the random integers generated by this line of code will be in the range [0, 1,115,386)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])#both of these will extract 8 numbers after that like we did above in the data and stack it making both a [4,8] tensor \n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb,yb=get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context=xb[b,:t+1]\n",
    "        target=yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target : {target}\")\n",
    "\n",
    "# so the size of xb will be[4,8] and each row contains 8 examples as we saw above in 1 so when we feed this tensor into\n",
    "# transformer we should get 4*8=32 examples to train on\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 65])\n",
      "None\n",
      "torch.Size([1, 65])\n",
      "tensor([0.0485, 0.0355, 0.0065, 0.0076, 0.0213, 0.0123, 0.0023, 0.0369, 0.0203,\n",
      "        0.0035, 0.0046, 0.0111, 0.0136, 0.0163, 0.0058, 0.0088, 0.0077, 0.0230,\n",
      "        0.0222, 0.0417, 0.0251, 0.0200, 0.0289, 0.0092, 0.0057, 0.0067, 0.0175,\n",
      "        0.0069, 0.0081, 0.0067, 0.0097, 0.0122, 0.0089, 0.0117, 0.0275, 0.0121,\n",
      "        0.0077, 0.0274, 0.0026, 0.0224, 0.0019, 0.0045, 0.0129, 0.0077, 0.0080,\n",
      "        0.0540, 0.0116, 0.0054, 0.0038, 0.0185, 0.0091, 0.0119, 0.0050, 0.0087,\n",
      "        0.0145, 0.0048, 0.0035, 0.0370, 0.0018, 0.0071, 0.0356, 0.0191, 0.0041,\n",
      "        0.0053, 0.0778], grad_fn=<AddBackward0>)\n",
      "tensor([[9]])\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):# idx is a 4*8 matrix of training data samples randomly\n",
    "        logits=self.token_embedding_table(idx)\n",
    "        if targets is None:# we are doing this just in case we want to extract logits as the function was earlier demanding targets , which means we will generate loss\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)#this will convert the 3d vector to 2d because when we use pytorch crossentropy it expects the input to be in the form minibatch*C\n",
    "            targets=targets.view(B*T)# so it will be 32*65 and targets will be 32*1, what this signifies is we are having 65 predictions to come after 32 tokens and in cross_entropy we are checkng how right we were \n",
    "            loss=F.cross_entropy(logits,targets)#comparing to targets \n",
    "\n",
    "        return logits,loss\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C), here we are plucking just the last character in idx, and taking its probabilities , giving it to probs and predicting next token\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            if _ == max_new_tokens-1:\n",
    "                print(logits.shape)\n",
    "                print(sum(probs))\n",
    "                print(idx_next)\n",
    "        return idx\n",
    "    \n",
    "m=BigramModel(vocab_size)\n",
    "logits,loss=m(xb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(m.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.572469472885132\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "for steps in range(10000):\n",
    "    xb,yb=get_batch('train')\n",
    "    logits,loss=m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65])\n",
      "tensor([1.8251e-02, 6.9886e-02, 1.8797e-03, 4.9633e-05, 2.1558e-04, 1.5814e-04,\n",
      "        6.2853e-04, 2.1214e-03, 2.0410e-04, 1.2496e-04, 1.3284e-03, 2.8734e-04,\n",
      "        2.1241e-03, 3.1556e-03, 3.3588e-03, 3.2946e-03, 1.9243e-03, 2.7700e-05,\n",
      "        3.0793e-03, 4.4056e-03, 3.3221e-03, 5.6676e-03, 7.9518e-04, 6.3333e-04,\n",
      "        8.4944e-04, 2.9489e-03, 9.6175e-05, 2.8589e-03, 2.6557e-03, 7.5182e-04,\n",
      "        2.7313e-03, 1.9270e-03, 3.0893e-02, 7.4418e-05, 1.8295e-03, 5.4527e-03,\n",
      "        6.2452e-04, 2.6289e-03, 2.6361e-03, 2.4434e-03, 4.2980e-03, 1.9629e-03,\n",
      "        2.3197e-01, 3.4531e-02, 2.5613e-03, 7.3800e-03, 3.0239e-03, 2.2798e-04,\n",
      "        1.1707e-03, 5.4094e-04, 8.1403e-02, 3.8865e-03, 5.7273e-03, 1.3215e-03,\n",
      "        2.5969e-03, 9.0154e-04, 7.7651e-03, 3.2721e-01, 8.5550e-02, 9.9976e-04,\n",
      "        1.4537e-03, 2.1062e-03, 1.2316e-04, 1.1619e-03, 1.7970e-03],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[57]])\n",
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercckehathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThineent.\n",
      "\n",
      "Lavinde.\n",
      "athave l.\n",
      "KEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers w dere! ABer wotouciullle's\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now from the above we got a accuracy of 2.57 using a bigram model , we were just using one character to predict the next.\n",
    "now what we want is the characters to interact with each other while predicting the next token , the below code is for the same. we are averaging out the prediction of a token to come next so the thrid character will be the mean of the first two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,32\n",
    "x=torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key=Linear(in_features=32, out_features=16, bias=False)\n",
      "query=Linear(in_features=32, out_features=16, bias=False)\n",
      "k=torch.Size([4, 8, 16])\n",
      "q=torch.Size([4, 8, 16])\n",
      "wei=tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3395, 0.6605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2869, 0.1655, 0.5475, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3727, 0.4912, 0.1089, 0.0271, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9357, 0.0246, 0.0092, 0.0117, 0.0188, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7632, 0.0689, 0.0110, 0.0082, 0.1422, 0.0065, 0.0000, 0.0000],\n",
      "         [0.0107, 0.0696, 0.0848, 0.4606, 0.0324, 0.1580, 0.1839, 0.0000],\n",
      "         [0.0994, 0.2195, 0.0995, 0.1397, 0.1052, 0.1152, 0.1307, 0.0908]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5242, 0.4758, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2482, 0.2221, 0.5298, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7299, 0.0198, 0.1748, 0.0756, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0824, 0.0146, 0.0300, 0.5735, 0.2995, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7646, 0.0143, 0.0550, 0.0048, 0.0099, 0.1513, 0.0000, 0.0000],\n",
      "         [0.7505, 0.0351, 0.0982, 0.0346, 0.0439, 0.0212, 0.0165, 0.0000],\n",
      "         [0.0544, 0.0495, 0.0141, 0.0882, 0.2664, 0.0814, 0.1022, 0.3438]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5368, 0.4632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0812, 0.4713, 0.4475, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7089, 0.0519, 0.0767, 0.1625, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2009, 0.1694, 0.1059, 0.2334, 0.2904, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1238, 0.6950, 0.1296, 0.0038, 0.0436, 0.0042, 0.0000, 0.0000],\n",
      "         [0.2646, 0.0553, 0.3554, 0.0019, 0.0786, 0.2387, 0.0055, 0.0000],\n",
      "         [0.0522, 0.1305, 0.0541, 0.2019, 0.0431, 0.2205, 0.2494, 0.0482]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0011, 0.9989, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1373, 0.6732, 0.1895, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5490, 0.0599, 0.3252, 0.0658, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2770, 0.1459, 0.3811, 0.1155, 0.0805, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1006, 0.1586, 0.0098, 0.1755, 0.3557, 0.1999, 0.0000, 0.0000],\n",
      "         [0.2402, 0.1085, 0.0661, 0.0519, 0.0387, 0.0149, 0.4798, 0.0000],\n",
      "         [0.0945, 0.0487, 0.0439, 0.0687, 0.1438, 0.1657, 0.1800, 0.2547]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size=16\n",
    "key=nn.Linear(C,head_size,bias=False)\n",
    "print(f'key={key}')\n",
    "query=nn.Linear(C,head_size,bias=False)\n",
    "print(f'query={query}')\n",
    "k=key(x)#(B,T,16) this basically means that earlier we had an embedding of 32 this will be reduced to 16 as we pass x through linear layer key so all in all each node will contain a key vector of 16 dimension \n",
    "print(f'k={k.shape}')\n",
    "q=query(x)#similar to above\n",
    "print(f'q={q.shape}')\n",
    "wei=q@k.transpose(-2,-1)# this means we are just transposing the last two dimension T and C so this will be (B,T,16)@(B,16,T) to give output of B,T,T\n",
    "# print(f'wei={wei[0]}')\n",
    "trill=torch.tril(torch.ones(T,T))\n",
    "# print(f'trill={trill}')\n",
    "# wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(trill==0,float('-inf'))\n",
    "# print(f'wei={wei[0]}')\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "print(f'wei={wei}')\n",
    "out=wei@x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6366, 0.3634, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3060, 0.3296, 0.3643, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1979, 0.2249, 0.2577, 0.3194, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1850, 0.3748, 0.1877, 0.1959, 0.0566, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1087, 0.0712, 0.0772, 0.0578, 0.3750, 0.3102, 0.0000, 0.0000],\n",
       "        [0.1439, 0.1283, 0.1566, 0.1666, 0.1521, 0.1075, 0.1450, 0.0000],\n",
       "        [0.0922, 0.2051, 0.0634, 0.0488, 0.0449, 0.3525, 0.0959, 0.0972]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above we were using bigram model , now we want the characters to interact with each other inorder to find out which character will come next. the best way to do so is to find out the average of all the character channels . we can do that by following below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trill=torch.tril(torch.ones(T,T))\n",
    "print(trill)\n",
    "wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(trill==0,float('-inf'))\n",
    "print(wei)\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "print(wei)\n",
    "xbow3=wei@x#what trill does is in a matrix it makes the upper half of the triangle equal to zeros , so it is like progressively taking the average of numbers one after another execute the first two lines to understandff\n",
    "torch.allclose(xbow,xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]]),\n",
       " tensor([[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[2],xbow3[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a=torch.tril(torch.ones(3,3))\n",
    "b=torch.randint(0,10,(3,2),).float()\n",
    "a=a/a.sum(1,keepdim=True)\n",
    "c=a@b\n",
    "print(f'a={a}')\n",
    "print(f'b={b}')\n",
    "print(f'c={c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei=torch.tril(torch.ones(T,T))\n",
    "wei=wei/wei.sum(1,keepdim=True)\n",
    "xbow2=wei@x#T,T mul B,T,C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(xbow2.shape)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
